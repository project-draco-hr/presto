{
  try {
    batchId++;
    long start=System.nanoTime();
    int batchSize=parquetReader.nextBatch();
    readTimeNanos+=System.nanoTime() - start;
    if (closed || batchSize <= 0) {
      close();
      return null;
    }
    Block[] blocks=new Block[hiveColumnIndexes.length];
    for (int fieldId=0; fieldId < blocks.length; fieldId++) {
      Type type=types.get(fieldId);
      if (constantBlocks[fieldId] != null) {
        blocks[fieldId]=constantBlocks[fieldId].getRegion(0,batchSize);
      }
 else {
        int fieldIndex=requestedSchema.getFieldIndex(columnNames.get(fieldId));
        ColumnDescriptor columnDescriptor=requestedSchema.getColumns().get(fieldIndex);
        blocks[fieldId]=new LazyBlock(batchSize,new ParquetBlockLoader(columnDescriptor,type));
      }
    }
    Page page=new Page(batchSize,blocks);
    long newCompletedBytes=(long)(totalBytes * parquetReader.getProgress());
    completedBytes=min(totalBytes,max(completedBytes,newCompletedBytes));
    return page;
  }
 catch (  PrestoException e) {
    closeWithSuppression(e);
    throw e;
  }
catch (  IOException|RuntimeException|InterruptedException e) {
    if (e instanceof InterruptedException) {
      Thread.currentThread().interrupt();
    }
    closeWithSuppression(e);
    throw new PrestoException(HIVE_CURSOR_ERROR,e);
  }
}
