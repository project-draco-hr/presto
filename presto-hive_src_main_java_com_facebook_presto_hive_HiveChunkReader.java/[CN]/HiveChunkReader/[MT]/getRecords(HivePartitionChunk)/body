{
  HadoopNative.requireHadoopNative();
  try {
    Properties schema=(Properties)chunk.getSchema().clone();
    String typeSpecification=(String)schema.remove(Constants.LIST_COLUMN_TYPES);
    Preconditions.checkNotNull(typeSpecification,"Partition column type specification is null");
    String nullSequence=(String)schema.get(Constants.SERIALIZATION_NULL_FORMAT);
    checkState(nullSequence == null || nullSequence.equals("\\N"),"Only '\\N' supported as null specifier, was '%s'",nullSequence);
    List<HiveColumnHandle> columns=ImmutableList.copyOf(filter(chunk.getColumns(),new Predicate<HiveColumnHandle>(){
      @Override public boolean apply(      HiveColumnHandle input){
        return !input.isPartitionKey();
      }
    }
));
    if (columns.isEmpty()) {
      columns=ImmutableList.of(getFirstPrimitiveColumn(chunk.getSchema()));
    }
    ColumnProjectionUtils.setReadColumnIDs(hdfsEnvironment.getConfiguration(),new ArrayList<>(transform(columns,hiveColumnIndexGetter())));
    RecordReader<?,?> recordReader=createRecordReader(chunk);
    if (recordReader.createValue() instanceof BytesRefArrayWritable) {
      return new BytesHiveRecordCursor<>((RecordReader<?,BytesRefArrayWritable>)recordReader,chunk.getLength(),chunk.getSchema(),chunk.getPartitionKeys(),columns);
    }
 else {
      return new GenericHiveRecordCursor<>((RecordReader<?,? extends Writable>)recordReader,chunk.getLength(),chunk.getSchema(),chunk.getPartitionKeys(),columns);
    }
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}
