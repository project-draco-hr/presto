{
  this.writableBucket=writableBucket;
  setupHive(databaseName);
  HiveClientConfig hiveClientConfig=new HiveClientConfig().setS3AwsAccessKey(awsAccessKey).setS3AwsSecretKey(awsSecretKey);
  String proxy=System.getProperty("hive.metastore.thrift.client.socks-proxy");
  if (proxy != null) {
    hiveClientConfig.setMetastoreSocksProxy(HostAndPort.fromString(proxy));
  }
  HiveConnectorId connectorId=new HiveConnectorId("hive-test");
  HiveCluster hiveCluster=new TestingHiveCluster(hiveClientConfig,host,port);
  ExecutorService executor=newCachedThreadPool(daemonThreadsNamed("hive-s3-%s"));
  HdfsConfiguration hdfsConfiguration=new HiveHdfsConfiguration(new HdfsConfigurationUpdater(hiveClientConfig));
  hdfsEnvironment=new HdfsEnvironment(hdfsConfiguration,hiveClientConfig);
  metastoreClient=new TestingHiveMetastore(hiveCluster,executor,hiveClientConfig,writableBucket);
  metadata=new HiveMetadata(connectorId,hiveClientConfig,metastoreClient,hdfsEnvironment,newDirectExecutorService(),new TypeRegistry());
  splitManager=new HiveSplitManager(connectorId,hiveClientConfig,metastoreClient,new NamenodeStats(),hdfsEnvironment,new HadoopDirectoryLister(),executor);
  recordSinkProvider=new HiveRecordSinkProvider(hdfsEnvironment);
  pageSourceProvider=new HivePageSourceProvider(hiveClientConfig,hdfsEnvironment,DEFAULT_HIVE_RECORD_CURSOR_PROVIDER,DEFAULT_HIVE_DATA_STREAM_FACTORIES,TYPE_MANAGER);
}
