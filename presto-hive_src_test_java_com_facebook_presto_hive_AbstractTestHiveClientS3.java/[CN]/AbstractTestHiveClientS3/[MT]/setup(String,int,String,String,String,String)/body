{
  this.writableBucket=writableBucket;
  setupHive(databaseName);
  HiveClientConfig hiveClientConfig=new HiveClientConfig().setS3AwsAccessKey(awsAccessKey).setS3AwsSecretKey(awsSecretKey);
  String proxy=System.getProperty("hive.metastore.thrift.client.socks-proxy");
  if (proxy != null) {
    hiveClientConfig.setMetastoreSocksProxy(HostAndPort.fromString(proxy));
  }
  HiveConnectorId connectorId=new HiveConnectorId("hive-test");
  HiveCluster hiveCluster=new TestingHiveCluster(hiveClientConfig,host,port);
  ExecutorService executor=newCachedThreadPool(daemonThreadsNamed("hive-s3-%s"));
  HdfsConfiguration hdfsConfiguration=new HiveHdfsConfiguration(new HdfsConfigurationUpdater(hiveClientConfig));
  HivePartitionManager hivePartitionManager=new HivePartitionManager(connectorId,TYPE_MANAGER,hiveClientConfig);
  hdfsEnvironment=new HdfsEnvironment(hdfsConfiguration,hiveClientConfig);
  metastoreClient=new TestingHiveMetastore(hiveCluster,executor,hiveClientConfig,writableBucket,hdfsEnvironment);
  locationService=new HiveLocationService(metastoreClient,hdfsEnvironment);
  TypeRegistry typeManager=new TypeRegistry();
  JsonCodec<PartitionUpdate> partitionUpdateCodec=JsonCodec.jsonCodec(PartitionUpdate.class);
  metadataFactory=new HiveMetadataFactory(connectorId,hiveClientConfig,metastoreClient,hdfsEnvironment,hivePartitionManager,newDirectExecutorService(),typeManager,locationService,new TableParameterCodec(),partitionUpdateCodec);
  splitManager=new HiveSplitManager(connectorId,hiveClientConfig,metastoreClient,new NamenodeStats(),hdfsEnvironment,new HadoopDirectoryLister(),executor);
  pageSinkProvider=new HivePageSinkProvider(hdfsEnvironment,metastoreClient,new GroupByHashPageIndexerFactory(),typeManager,new HiveClientConfig(),locationService,partitionUpdateCodec);
  pageSourceProvider=new HivePageSourceProvider(hiveClientConfig,hdfsEnvironment,getDefaultHiveRecordCursorProvider(hiveClientConfig),getDefaultHiveDataStreamFactories(hiveClientConfig),TYPE_MANAGER);
}
