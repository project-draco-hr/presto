{
  checkNotNull(databaseName,"databaseName is null");
  checkNotNull(tableName,"tableName is null");
  checkNotNull(partitionName,"partitionName is null");
  if (hiveImportRegistry.isPartitionImported(databaseName,tableName,partitionName)) {
    return 0;
  }
  List<PartitionChunk> chunks=runWithRetry(new Callable<List<PartitionChunk>>(){
    @Override public List<PartitionChunk> call() throws Exception {
      return hiveClient.getPartitionChunks(databaseName,tableName,partitionName);
    }
  }
,databaseName + "." + tableName+ "."+ partitionName+ ".getPartitionChunks");
  final List<SchemaField> schemaFields=runWithRetry(new Callable<List<SchemaField>>(){
    @Override public List<SchemaField> call() throws Exception {
      return hiveClient.getTableSchema(databaseName,tableName);
    }
  }
,databaseName + "." + tableName+ "."+ partitionName+ ".getTableSchema");
synchronized (this) {
    String catalogName="default";
    String schemaName="default";
    if (metadata.getTable(catalogName,schemaName,tableName) == null) {
      List<ColumnMetadata> columns=createColumnMetadata(schemaFields);
      metadata.createTable(new TableMetadata(catalogName,schemaName,tableName,columns));
    }
  }
  long rowCount=0;
  for (  final PartitionChunk chunk : chunks) {
    rowCount+=runWithRetry(new Callable<Long>(){
      @Override public Long call() throws Exception {
        try (RecordIterator records=hiveClient.getRecords(chunk)){
          TupleStream sourceTupleStream=new HiveTupleStream(records,schemaFields);
          return storageManager.importTableShard(sourceTupleStream,databaseName,tableName);
        }
       }
    }
,databaseName + "." + tableName+ "."+ partitionName+ "."+ chunk+ ".import");
  }
  hiveImportRegistry.markPartitionImported(databaseName,tableName,partitionName);
  return rowCount;
}
