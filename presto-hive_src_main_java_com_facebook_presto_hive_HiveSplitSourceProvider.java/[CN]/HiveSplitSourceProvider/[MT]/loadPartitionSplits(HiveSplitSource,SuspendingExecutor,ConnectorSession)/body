{
  final Semaphore semaphore=new Semaphore(maxPartitionBatchSize);
  try (ThreadContextClassLoader ignored=new ThreadContextClassLoader(classLoader)){
    ImmutableList.Builder<ListenableFuture<Void>> futureBuilder=ImmutableList.builder();
    for (    HivePartitionMetadata partition : partitions) {
      final String partitionName=partition.getHivePartition().getPartitionId();
      final Properties schema=getPartitionSchema(table,partition.getPartition());
      final List<HivePartitionKey> partitionKeys=getPartitionKeys(table,partition.getPartition());
      final TupleDomain<HiveColumnHandle> tupleDomain=(TupleDomain<HiveColumnHandle>)(Object)partition.getHivePartition().getTupleDomain();
      Path path=new Path(getPartitionLocation(table,partition.getPartition()));
      Configuration configuration=hdfsEnvironment.getConfiguration(path);
      final InputFormat<?,?> inputFormat=getInputFormat(configuration,schema,false);
      if (inputFormat instanceof SymlinkTextInputFormat) {
        JobConf jobConf=new JobConf(configuration);
        FileInputFormat.setInputPaths(jobConf,path);
        InputSplit[] splits=inputFormat.getSplits(jobConf,0);
        for (        InputSplit rawSplit : splits) {
          FileSplit split=((SymlinkTextInputFormat.SymlinkTextInputSplit)rawSplit).getTargetSplit();
          FileSystem targetFilesystem=hdfsEnvironment.getFileSystem(split.getPath());
          FileStatus fileStatus=targetFilesystem.getFileStatus(split.getPath());
          hiveSplitSource.addToQueue(createHiveSplits(partitionName,fileStatus,targetFilesystem.getFileBlockLocations(fileStatus,split.getStart(),split.getLength()),split.getStart(),split.getLength(),schema,partitionKeys,false,session,partition.getHivePartition().getEffectivePredicate()));
        }
        continue;
      }
      FileSystem fs=hdfsEnvironment.getFileSystem(path);
      if (bucket.isPresent()) {
        Optional<FileStatus> bucketFile=getBucketFile(bucket.get(),fs,path);
        if (bucketFile.isPresent()) {
          FileStatus file=bucketFile.get();
          BlockLocation[] blockLocations=fs.getFileBlockLocations(file,0,file.getLen());
          boolean splittable=isSplittable(inputFormat,fs,file.getPath());
          hiveSplitSource.addToQueue(createHiveSplits(partitionName,file,blockLocations,0,file.getLen(),schema,partitionKeys,splittable,session,tupleDomain));
          continue;
        }
      }
      try {
        semaphore.acquire();
      }
 catch (      InterruptedException e) {
        Thread.currentThread().interrupt();
        return;
      }
      ListenableFuture<Void> partitionFuture=createAsyncWalker(fs,suspendingExecutor).beginWalk(path,new FileStatusCallback(){
        @Override public void process(        FileStatus file,        BlockLocation[] blockLocations){
          try {
            boolean splittable=isSplittable(inputFormat,hdfsEnvironment.getFileSystem(file.getPath()),file.getPath());
            hiveSplitSource.addToQueue(createHiveSplits(partitionName,file,blockLocations,0,file.getLen(),schema,partitionKeys,splittable,session,tupleDomain));
          }
 catch (          IOException e) {
            hiveSplitSource.fail(e);
          }
        }
      }
);
      Futures.addCallback(partitionFuture,new FutureCallback<Void>(){
        @Override public void onSuccess(        Void result){
          semaphore.release();
        }
        @Override public void onFailure(        Throwable t){
          semaphore.release();
        }
      }
);
      futureBuilder.add(partitionFuture);
    }
    Futures.addCallback(Futures.allAsList(futureBuilder.build()),new FutureCallback<List<Void>>(){
      @Override public void onSuccess(      List<Void> result){
        hiveSplitSource.finished();
      }
      @Override public void onFailure(      Throwable t){
        hiveSplitSource.fail(t);
      }
    }
);
  }
 catch (  Throwable e) {
    hiveSplitSource.fail(e);
    Throwables.propagateIfInstanceOf(e,Error.class);
  }
}
