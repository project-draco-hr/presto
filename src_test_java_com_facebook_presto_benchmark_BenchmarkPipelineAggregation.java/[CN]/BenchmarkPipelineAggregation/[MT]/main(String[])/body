{
  File groupByFile=new File("data/column5/column0.data");
  File aggregateFile=new File("data/columns/column3.data");
  Slice groupBySlice=Slices.mapFileReadOnly(groupByFile);
  Slice aggregateSlice=Slices.mapFileReadOnly(aggregateFile);
  for (int i=0; i < 100000; ++i) {
    TupleStream groupBySource=UncompressedSerde.readAsStream(groupBySlice);
    TupleStream aggregateSource=UncompressedSerde.readAsStream(aggregateSlice);
    GroupByOperator groupBy=new GroupByOperator(groupBySource);
    PipelinedAggregationOperator aggregation=new PipelinedAggregationOperator(groupBy,aggregateSource,SumAggregation.PROVIDER);
    Result result=doIt(aggregation);
    long count=result.count;
    Duration duration=result.duration;
    DataSize fileSize=new DataSize(groupByFile.length() + aggregateFile.length(),DataSize.Unit.BYTE);
    System.out.println(String.format("%s, %s, %.2f/s, %2.2f MB/s",duration,count,count / duration.toMillis() * 1000,fileSize.getValue(DataSize.Unit.MEGABYTE) / duration.convertTo(TimeUnit.SECONDS)));
  }
  Thread.sleep(1000);
}
