{
  setupHive(connectorName,databaseName,timeZoneId);
  HiveClientConfig hiveClientConfig=new HiveClientConfig();
  hiveClientConfig.setTimeZone(timeZoneId);
  String proxy=System.getProperty("hive.metastore.thrift.client.socks-proxy");
  if (proxy != null) {
    hiveClientConfig.setMetastoreSocksProxy(HostAndPort.fromString(proxy));
  }
  HiveCluster hiveCluster=new TestingHiveCluster(hiveClientConfig,host,port);
  metastoreClient=new CachingHiveMetastore(hiveCluster,executor,Duration.valueOf("1m"),Duration.valueOf("15s"));
  HiveConnectorId connectorId=new HiveConnectorId(connectorName);
  HdfsConfiguration hdfsConfiguration=new HiveHdfsConfiguration(new HdfsConfigurationUpdater(hiveClientConfig));
  hdfsEnvironment=new HdfsEnvironment(hdfsConfiguration,hiveClientConfig);
  locationService=new HiveLocationService(metastoreClient,hdfsEnvironment);
  TypeManager typeManager=new TypeRegistry();
  JsonCodec<PartitionUpdate> partitionUpdateCodec=JsonCodec.jsonCodec(PartitionUpdate.class);
  metadataFactory=new HiveMetadataFactory(connectorId,metastoreClient,hdfsEnvironment,new HivePartitionManager(connectorId,TYPE_MANAGER,hiveClientConfig),timeZone,10,true,true,HiveStorageFormat.RCBINARY,typeManager,locationService,new TableParameterCodec(),partitionUpdateCodec,newFixedThreadPool(2));
  splitManager=new HiveSplitManager(connectorId,metastoreClient,new NamenodeStats(),hdfsEnvironment,new HadoopDirectoryLister(),newDirectExecutorService(),maxOutstandingSplits,hiveClientConfig.getMinPartitionBatchSize(),hiveClientConfig.getMaxPartitionBatchSize(),hiveClientConfig.getMaxInitialSplits(),false);
  pageSinkProvider=new HivePageSinkProvider(hdfsEnvironment,metastoreClient,new GroupByHashPageIndexerFactory(),typeManager,new HiveClientConfig(),locationService,partitionUpdateCodec);
  pageSourceProvider=new HivePageSourceProvider(hiveClientConfig,hdfsEnvironment,getDefaultHiveRecordCursorProvider(hiveClientConfig),getDefaultHiveDataStreamFactories(hiveClientConfig),TYPE_MANAGER);
}
