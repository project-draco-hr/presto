{
  final Semaphore semaphore=new Semaphore(maxPartitionBatchSize);
  try (ThreadContextClassLoader ignored=new ThreadContextClassLoader(classLoader)){
    ImmutableList.Builder<ListenableFuture<Void>> futureBuilder=ImmutableList.builder();
    Iterator<String> nameIterator=partitionNames.iterator();
    for (    org.apache.hadoop.hive.metastore.api.Partition partition : partitions) {
      checkState(nameIterator.hasNext(),"different number of partitions and partition names!");
      semaphore.acquire();
      final String partitionName=nameIterator.next();
      final Properties schema=getPartitionSchema(table,partition);
      final List<HivePartitionKey> partitionKeys=getPartitionKeys(table,partition);
      Path path=new Path(getPartitionLocation(table,partition));
      final Configuration configuration=hdfsEnvironment.getConfiguration(path);
      final InputFormat<?,?> inputFormat=getInputFormat(configuration,schema,false);
      Path partitionPath=hdfsEnvironment.getFileSystemWrapper().wrap(path);
      FileSystem fs=partitionPath.getFileSystem(configuration);
      final LastSplitMarkingQueue markerQueue=new LastSplitMarkingQueue(hiveSplitQueue);
      if (inputFormat instanceof SymlinkTextInputFormat) {
        JobConf jobConf=new JobConf(configuration);
        FileInputFormat.setInputPaths(jobConf,partitionPath);
        InputSplit[] splits=inputFormat.getSplits(jobConf,0);
        for (        InputSplit rawSplit : splits) {
          FileSplit split=((SymlinkTextInputSplit)rawSplit).getTargetSplit();
          FileSystem targetFilesystem=split.getPath().getFileSystem(configuration);
          FileStatus fileStatus=targetFilesystem.getFileStatus(split.getPath());
          markerQueue.addToQueue(createHiveSplits(partitionName,fileStatus,targetFilesystem.getFileBlockLocations(fileStatus,split.getStart(),split.getLength()),split.getStart(),split.getLength(),schema,partitionKeys,false));
        }
        markerQueue.finish();
        continue;
      }
      ListenableFuture<Void> partitionFuture=new AsyncRecursiveWalker(fs,suspendingExecutor).beginWalk(partitionPath,new FileStatusCallback(){
        @Override public void process(        FileStatus file,        BlockLocation[] blockLocations){
          try {
            boolean splittable=isSplittable(inputFormat,file.getPath().getFileSystem(configuration),file.getPath());
            markerQueue.addToQueue(createHiveSplits(partitionName,file,blockLocations,0,file.getLen(),schema,partitionKeys,splittable));
          }
 catch (          IOException e) {
            hiveSplitQueue.fail(e);
          }
        }
      }
);
      Futures.addCallback(partitionFuture,new FutureCallback<Void>(){
        @Override public void onSuccess(        Void result){
          markerQueue.finish();
          semaphore.release();
        }
        @Override public void onFailure(        Throwable t){
          markerQueue.finish();
          semaphore.release();
        }
      }
);
      futureBuilder.add(partitionFuture);
    }
    Futures.addCallback(Futures.allAsList(futureBuilder.build()),new FutureCallback<List<Void>>(){
      @Override public void onSuccess(      List<Void> result){
        hiveSplitQueue.finished();
      }
      @Override public void onFailure(      Throwable t){
        hiveSplitQueue.fail(t);
      }
    }
);
  }
 catch (  Throwable e) {
    hiveSplitQueue.fail(e);
    Throwables.propagateIfInstanceOf(e,Error.class);
  }
}
