{
  Properties schema=MetaStoreUtils.getSchema(partition,table);
  List<HivePartitionKey> partitionKeys=getPartitionKeys(table,partition);
  InputFormat inputFormat=getInputFormat(schema);
  Path partitionLocation=new CachingPath(partition.getSd().getLocation());
  FileSystem fs=partitionLocation.getFileSystem(HADOOP_CONFIGURATION.get());
  ImmutableList.Builder<PartitionChunk> list=ImmutableList.builder();
  List<FileStatus> files=new RecursiveFileSystemTraversal(fs,partitionLocation,HDFS_LIST_EXECUTOR).list();
  for (  FileStatus file : files) {
    boolean splittable=isSplittable(inputFormat,file.getPath().getFileSystem(HADOOP_CONFIGURATION.get()),file.getPath());
    long splitSize=splittable ? maxChunkBytes : file.getLen();
    for (long start=0; start < file.getLen(); start+=splitSize) {
      long length=min(splitSize,file.getLen() - start);
      list.add(new HivePartitionChunk(file.getPath(),start,length,schema,partitionKeys,columns));
    }
  }
  return list.build();
}
