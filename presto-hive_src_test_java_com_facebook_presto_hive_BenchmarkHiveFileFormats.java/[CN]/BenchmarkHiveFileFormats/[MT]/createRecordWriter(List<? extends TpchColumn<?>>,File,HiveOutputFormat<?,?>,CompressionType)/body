{
  JobConf jobConf=new JobConf();
  ReaderWriterProfiler.setProfilerOptions(jobConf);
  if (compressionCodec != CompressionType.none) {
    CompressionCodec codec=new CompressionCodecFactory(new Configuration()).getCodecByName(compressionCodec.toString());
    jobConf.set(COMPRESS_CODEC,codec.getClass().getName());
    jobConf.set(COMPRESS_TYPE,org.apache.hadoop.io.SequenceFile.CompressionType.BLOCK.toString());
    jobConf.set("parquet.compression",compressionCodec.toString());
    jobConf.set("parquet.enable.dictionary","true");
switch (compressionCodec) {
case gzip:
      jobConf.set("hive.exec.orc.default.compress","ZLIB");
    jobConf.set("hive.exec.orc.compress","ZLIB");
  break;
case snappy:
jobConf.set("hive.exec.orc.default.compress","SNAPPY");
jobConf.set("hive.exec.orc.compress","SNAPPY");
break;
default :
throw new IllegalArgumentException("Unsupported compression codec: " + compressionCodec);
}
}
 else {
jobConf.set("parquet.enable.dictionary","true");
jobConf.set("hive.exec.orc.default.compress","NONE");
jobConf.set("hive.exec.orc.compress","NONE");
}
RecordWriter recordWriter=outputFormat.getHiveRecordWriter(jobConf,new Path(outputFile.toURI()),Text.class,compressionCodec != CompressionType.none,createTableProperties(columns),new Progressable(){
@Override public void progress(){
}
}
);
return recordWriter;
}
