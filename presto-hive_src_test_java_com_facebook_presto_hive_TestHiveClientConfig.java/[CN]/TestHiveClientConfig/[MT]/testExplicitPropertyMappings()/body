{
  Map<String,String> properties=new ImmutableMap.Builder<String,String>().put("hive.max-split-size","256MB").put("hive.max-outstanding-splits","10").put("hive.max-global-split-iterator-threads","10").put("hive.max-split-iterator-threads","2").put("hive.metastore-cache-ttl","2h").put("hive.metastore-refresh-interval","30m").put("hive.metastore-refresh-max-threads","2500").put("hive.metastore.thrift.client.socks-proxy","localhost:1080").put("hive.metastore-timeout","20s").put("hive.metastore.partition-batch-size.min","1").put("hive.metastore.partition-batch-size.max","1000").put("hive.dfs-timeout","33s").put("hive.dfs.connect.timeout","20s").put("hive.dfs.connect.max-retries","10").put("hive.file-system-cache-ttl","2d").put("hive.config.resources","/foo.xml,/bar.xml").put("dfs.domain-socket-path","/foo").put("hive.s3.aws-access-key","abc123").put("hive.s3.aws-secret-key","secret").build();
  HiveClientConfig expected=new HiveClientConfig().setMaxSplitSize(new DataSize(256,Unit.MEGABYTE)).setMaxOutstandingSplits(10).setMaxGlobalSplitIteratorThreads(10).setMaxSplitIteratorThreads(2).setMetastoreCacheTtl(new Duration(2,TimeUnit.HOURS)).setMetastoreRefreshInterval(new Duration(30,TimeUnit.MINUTES)).setMaxMetastoreRefreshThreads(2500).setMetastoreSocksProxy(HostAndPort.fromParts("localhost",1080)).setMetastoreTimeout(new Duration(20,TimeUnit.SECONDS)).setMinPartitionBatchSize(1).setMaxPartitionBatchSize(1000).setDfsTimeout(new Duration(33,TimeUnit.SECONDS)).setDfsConnectTimeout(new Duration(20,TimeUnit.SECONDS)).setDfsConnectMaxRetries(10).setFileSystemCacheTtl(new Duration(2,TimeUnit.DAYS)).setResourceConfigFiles(ImmutableList.of("/foo.xml","/bar.xml")).setDomainSocketPath("/foo").setS3AwsAccessKey("abc123").setS3AwsSecretKey("secret");
  ConfigAssertions.assertFullMapping(properties,expected);
}
