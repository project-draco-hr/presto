{
  String partitionName=partition.getHivePartition().getPartitionId();
  Properties schema=getPartitionSchema(table,partition.getPartition());
  List<HivePartitionKey> partitionKeys=getPartitionKeys(table,partition.getPartition());
  TupleDomain<HiveColumnHandle> effectivePredicate=partition.getHivePartition().getEffectivePredicate();
  Path path=new Path(getPartitionLocation(table,partition.getPartition()));
  Configuration configuration=hdfsEnvironment.getConfiguration(path);
  InputFormat<?,?> inputFormat=getInputFormat(configuration,schema,false);
  if (inputFormat instanceof SymlinkTextInputFormat) {
    if (bucketHandle.isPresent()) {
      throw new PrestoException(StandardErrorCode.NOT_SUPPORTED,"Bucketed table in SymlinkTextInputFormat is not yet supported");
    }
    JobConf jobConf=new JobConf(configuration);
    FileInputFormat.setInputPaths(jobConf,path);
    InputSplit[] splits=inputFormat.getSplits(jobConf,0);
    for (    InputSplit rawSplit : splits) {
      FileSplit split=((SymlinkTextInputFormat.SymlinkTextInputSplit)rawSplit).getTargetSplit();
      FileSystem targetFilesystem=hdfsEnvironment.getFileSystem(split.getPath());
      FileStatus file=targetFilesystem.getFileStatus(split.getPath());
      hiveSplitSource.addToQueue(createHiveSplits(partitionName,file.getPath().toString(),targetFilesystem.getFileBlockLocations(file,split.getStart(),split.getLength()),split.getStart(),split.getLength(),schema,partitionKeys,false,session,OptionalInt.empty(),effectivePredicate));
      if (stopped) {
        return;
      }
    }
    return;
  }
  FileSystem fs=hdfsEnvironment.getFileSystem(path);
  HiveFileIterator iterator=new HiveFileIterator(path,fs,directoryLister,namenodeStats,partitionName,inputFormat,schema,partitionKeys,effectivePredicate);
  if (bucket.isPresent()) {
    List<LocatedFileStatus> locatedFileStatuses=listAndSortBucketFiles(iterator,bucket.get().getBucketCount());
    FileStatus file=locatedFileStatuses.get(bucket.get().getBucketNumber());
    BlockLocation[] blockLocations=fs.getFileBlockLocations(file,0,file.getLen());
    boolean splittable=isSplittable(inputFormat,fs,file.getPath());
    hiveSplitSource.addToQueue(createHiveSplits(partitionName,file.getPath().toString(),blockLocations,0,file.getLen(),schema,partitionKeys,splittable,session,OptionalInt.of(bucket.get().getBucketNumber()),effectivePredicate));
    return;
  }
  if (bucketHandle.isPresent()) {
    int bucketCount=bucketHandle.get().getBucketCount();
    List<LocatedFileStatus> list=listAndSortBucketFiles(iterator,bucketCount);
    for (int bucketIndex=0; bucketIndex < bucketCount; bucketIndex++) {
      LocatedFileStatus file=list.get(bucketIndex);
      boolean splittable=isSplittable(iterator.getInputFormat(),hdfsEnvironment.getFileSystem(file.getPath()),file.getPath());
      hiveSplitSource.addToQueue(createHiveSplits(iterator.getPartitionName(),file.getPath().toString(),file.getBlockLocations(),0,file.getLen(),iterator.getSchema(),iterator.getPartitionKeys(),splittable,session,OptionalInt.of(bucketIndex),iterator.getEffectivePredicate()));
    }
    return;
  }
  fileIterators.addLast(iterator);
}
