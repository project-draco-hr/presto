{
  String partitionName=partition.getHivePartition().getPartitionId();
  Properties schema=getPartitionSchema(table,partition.getPartition());
  List<HivePartitionKey> partitionKeys=getPartitionKeys(table,partition.getPartition());
  TupleDomain<HiveColumnHandle> effectivePredicate=partition.getHivePartition().getEffectivePredicate();
  Path path=new Path(getPartitionLocation(table,partition.getPartition()));
  Configuration configuration=hdfsEnvironment.getConfiguration(path);
  InputFormat<?,?> inputFormat=getInputFormat(configuration,schema,false);
  if (inputFormat instanceof SymlinkTextInputFormat) {
    if (bucketHandle.isPresent()) {
      throw new PrestoException(StandardErrorCode.NOT_SUPPORTED,"Bucketed table in SymlinkTextInputFormat is not yet supported");
    }
    for (    Path targetPath : getTargetPathsFromSymlink(configuration,path)) {
      TextInputFormat targetInputFormat=new TextInputFormat();
      Configuration targetConfiguration=hdfsEnvironment.getConfiguration(targetPath);
      JobConf targetJob=new JobConf(targetConfiguration);
      targetJob.setInputFormat(TextInputFormat.class);
      targetInputFormat.configure(targetJob);
      FileInputFormat.setInputPaths(targetJob,targetPath);
      InputSplit[] targetSplits=targetInputFormat.getSplits(targetJob,0);
      for (      InputSplit inputSplit : targetSplits) {
        FileSplit split=(FileSplit)inputSplit;
        FileSystem targetFilesystem=split.getPath().getFileSystem(targetConfiguration);
        FileStatus file=targetFilesystem.getFileStatus(split.getPath());
        hiveSplitSource.addToQueue(createHiveSplits(partitionName,file.getPath().toString(),targetFilesystem.getFileBlockLocations(file,split.getStart(),split.getLength()),split.getStart(),split.getLength(),schema,partitionKeys,false,session,OptionalInt.empty(),effectivePredicate));
        if (stopped) {
          return;
        }
      }
    }
    return;
  }
  FileSystem fs=hdfsEnvironment.getFileSystem(path);
  HiveFileIterator iterator=new HiveFileIterator(path,fs,directoryLister,namenodeStats,partitionName,inputFormat,schema,partitionKeys,effectivePredicate);
  if (bucket.isPresent()) {
    List<LocatedFileStatus> locatedFileStatuses=listAndSortBucketFiles(iterator,bucket.get().getBucketCount());
    FileStatus file=locatedFileStatuses.get(bucket.get().getBucketNumber());
    BlockLocation[] blockLocations=fs.getFileBlockLocations(file,0,file.getLen());
    boolean splittable=isSplittable(inputFormat,fs,file.getPath());
    hiveSplitSource.addToQueue(createHiveSplits(partitionName,file.getPath().toString(),blockLocations,0,file.getLen(),schema,partitionKeys,splittable,session,OptionalInt.of(bucket.get().getBucketNumber()),effectivePredicate));
    return;
  }
  if (bucketHandle.isPresent()) {
    int bucketCount=bucketHandle.get().getBucketCount();
    List<LocatedFileStatus> list=listAndSortBucketFiles(iterator,bucketCount);
    for (int bucketIndex=0; bucketIndex < bucketCount; bucketIndex++) {
      LocatedFileStatus file=list.get(bucketIndex);
      boolean splittable=isSplittable(iterator.getInputFormat(),hdfsEnvironment.getFileSystem(file.getPath()),file.getPath());
      hiveSplitSource.addToQueue(createHiveSplits(iterator.getPartitionName(),file.getPath().toString(),file.getBlockLocations(),0,file.getLen(),iterator.getSchema(),iterator.getPartitionKeys(),splittable,session,OptionalInt.of(bucketIndex),iterator.getEffectivePredicate()));
    }
    return;
  }
  fileIterators.addLast(iterator);
}
